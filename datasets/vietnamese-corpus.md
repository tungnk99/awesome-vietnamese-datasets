# 📚 Vietnamese Corpus Datasets


Tài liệu này tổng hợp các **corpus tiếng Việt** phục vụ cho mục đích nghiên cứu và phát triển trong lĩnh vực Natural Language Processing (NLP).  
Các corpus chủ yếu bao gồm tập văn bản thô, được sử dụng làm nguồn dữ liệu cho quá trình training và pre-training các language models (LM).

---

## 🔹 General Corpus
Văn bản chung (Wikipedia, báo chí, sách, web…)

| Dataset | #Samples | Size | Year | License | Link | Citation |
|---------|----------|------|------|---------|------|----------|
| wikipedia_vi | 1,281,412 | ~570MB | – | – | [Hugging Face](https://huggingface.co/datasets/vietgpt/wikipedia_vi) | – |
| Vietnamese-Book-Corpus | 16,407 | ~1.92GB | – | – | [Hugging Face](https://huggingface.co/datasets/tmnam20/Vietnamese-Book-Corpus) | – |
| Binhvq News Corpus | 14,896,998 | ~18.6 GB | 2018 | Apache License | [GitHub](https://github.com/binhvq/news-corpus) | – |
| vietnamese-poetry-corpus | 198,598 | ~164 MB | –  | – | [Hugging Face](https://huggingface.co/datasets/phamson02/vietnamese-poetry-corpus) | – |
| BKAINewsCorpus | 16,762,024 | ~28.7 GB | 2024 | – | [Hugging Face](https://huggingface.co/datasets/bkai-foundation-models/BKAINewsCorpus) | – |
| NewsCategory  | 596,524 | ~1.04 GB | 2024 | – | [Hugging Face](https://huggingface.co/datasets/bkai-foundation-models/NewsCategory) | – |
| NewsSapo   | 31,728,183 | - | 2024 | – | [Hugging Face](https://huggingface.co/datasets/bkai-foundation-models/NewsSapo) | – |
| 10000 Vietnamese Books | 10400 | ~8.5 GB |  –  | – | [kaggle](https://www.kaggle.com/datasets/iambestfeeder/10000-vietnamese-books) | – |
| VietVault |  –  | ~80GB |  –  | – | [Hugging Face](https://huggingface.co/datasets/nampdn-ai/vietvault) | – |

**📝 Notes:**  
- *wikipedia_vi*: 
- *Vietnamese-Book-Corpus*: 
- *Binhvq News Corpus*:
- *vietnamese-poetry-corpus*:
- *BKAINewsCorpus*:
- *NewsCategory*:
- *NewsSapo*:


---

👉 **Lưu ý chung:**  
- Các con số chỉ mang tính tham khảo, cần kiểm chứng từ nguồn chính thức.  
- Một số dataset bị giới hạn bản quyền → chỉ dùng cho mục đích nghiên cứu.  
- Một số dataset có thể trùng lặp dữ liệu với nhau, cần xử lý loại bỏ duplicate khi sử dụng
- Bạn có thể đóng góp thêm corpus mới bằng cách tạo **Pull Request** 🙌.  
